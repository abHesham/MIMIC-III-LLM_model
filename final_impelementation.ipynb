{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BqTakzYPKa3n"},"outputs":[],"source":["#run this code\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ARRqRjZlED-"},"outputs":[],"source":["#run this code\n","!pip install happytransformer\n","!pip install lime\n","!pip install shap"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BCUeODsAFVs4"},"outputs":[],"source":["# #tokenizing\n","# from transformers import AutoTokenizer\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","# train_tokens = tokenizer(list(train['TEXT']), padding=True, truncation=True, return_tensors='pt', max_length=500)\n","# validation_tokens = tokenizer(list(validation['TEXT']), padding=True, truncation=True, return_tensors='pt', max_length=500)\n","# test_tokens = tokenizer(list(test['TEXT']), padding=True, truncation=True, return_tensors='pt', max_length=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yQZ50rNtFVs4"},"outputs":[],"source":["# # Save tokenized data\n","# torch.save(train_tokens, 'train_tokens.pt')\n","# torch.save(validation_tokens, 'validation_tokens.pt')\n","# torch.save(test_tokens, 'test_tokens.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SzJ8etKwFVs4"},"outputs":[],"source":["# # Load tokenized data\n","# loaded_train_tokens = torch.load('train_tokens.pt')\n","# loaded_test_tokens = torch.load('test_tokens.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eTjRLYQyFVs4"},"outputs":[],"source":["# #Model\n","# from transformers import AutoModelForSequenceClassification, AdamW\n","\n","# model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NmKyp8-nFVs4"},"outputs":[],"source":["# #Training\n","# from torch.utils.data import DataLoader, TensorDataset\n","# from torch.optim import AdamW\n","\n","\n","# learning_rate = 1e-3\n","# batch_size = 10\n","# epochs = 9\n","\n","# train_dataset = TensorDataset(train_tokens['input_ids'], train_tokens['attention_mask'], torch.tensor(list(train['ICD9_CODE_encoded'])))\n","# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","# for epoch in range(epochs):\n","#     for batch in train_loader:\n","#         optimizer.zero_grad()\n","#         inputs, attention_mask, labels = batch\n","#         outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n","#         loss = outputs.loss\n","#         loss.backward()\n","#         optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BwS1it94FVs4"},"outputs":[],"source":["# #Evaluation\n","# from sklearn.metrics import classification_report\n","\n","# with torch.no_grad():\n","#     model.eval()\n","#     predictions = model(test_tokens['input_ids'], attention_mask=test_tokens['attention_mask']).logits.argmax(dim=1)\n","\n","# print(classification_report(test_labels, predictions))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xtlnr8u9koC4"},"outputs":[],"source":["##using happytransformer\n","\n","# from happytransformer import HappyTextClassification, TCTrainArgs\n","# from torchinfo import summary\n","# # --------------------------------------#\n","# happy_tc = HappyTextClassification(model_type=\"BERT\",\n","#                                    model_name=\"medicalai/ClinicalBERT\",\n","#                                    num_labels=2)\n","\n","# args = TCTrainArgs(learning_rate = 1e-5, batch_size = 32, num_train_epochs = 9, save_steps= 0.1, output_dir = \"/content/drive/MyDrive/models_clinicalBERT\")\n","# training = happy_tc.train(\"/content/drive/MyDrive/2_whole_dataset_train_validate_df_100.csv\", args=args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AX5JSBFimo3"},"outputs":[],"source":["# # #using happytransformer\n","# # #using BioBERT\n","# from happytransformer import HappyTextClassification, TCTrainArgs\n","# from torchinfo import summary\n","# # --------------------------------------#\n","# happy_tc = HappyTextClassification(model_type=\"BioBERT\", model_name=\"emilyalsentzer/Bio_ClinicalBERT\",\n","#                                  num_labels=2)\n","\n","# # Train the model\n","# args = TCTrainArgs(learning_rate = 1e-6, batch_size = 32, num_train_epochs = 5, save_steps= 0.1, output_dir = \"/content/drive/MyDrive/models_BIOBERT\")\n","# happy_tc.train('/content/drive/MyDrive/whole_dataset/2_preprocessed_train_validate_df_60.csv', args=args)\n","# happy_tc.save(\"finetuned_model/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2g1FB5Vy-TZ"},"outputs":[],"source":["#run this code\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import re\n","import torch\n","import shap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5LOjYZBzDjF"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/whole_dataset/final_dataset_5.csv', low_memory=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1704227207092,"user":{"displayName":"project","userId":"11180814093033221546"},"user_tz":-120},"id":"XM1iLfrMzOS6","outputId":"bd45709f-d792-4642-e2b3-f8f11dcb9834"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-b45f4928-00a7-4aed-8035-6117f7201b44\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>SUBJECT_ID</th>\n","      <th>ICD9_CODE</th>\n","      <th>TEXT</th>\n","      <th>GENDER</th>\n","      <th>clean_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>958</td>\n","      <td>112</td>\n","      <td>41401</td>\n","      <td>Admission Date: [**2194-6-13**]        Dischar...</td>\n","      <td>M</td>\n","      <td>948</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>959</td>\n","      <td>112</td>\n","      <td>41401</td>\n","      <td>Admission Date:  [**2196-9-27**]              ...</td>\n","      <td>M</td>\n","      <td>881</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>965</td>\n","      <td>114</td>\n","      <td>41401</td>\n","      <td>Admission Date:  [**2146-8-29**]       Dischar...</td>\n","      <td>M</td>\n","      <td>558</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>980</td>\n","      <td>115</td>\n","      <td>4280</td>\n","      <td>Admission Date:  [**2194-10-16**]             ...</td>\n","      <td>F</td>\n","      <td>1552</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>982</td>\n","      <td>115</td>\n","      <td>4019</td>\n","      <td>Admission Date:  [**2194-10-16**]             ...</td>\n","      <td>F</td>\n","      <td>1552</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b45f4928-00a7-4aed-8035-6117f7201b44')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b45f4928-00a7-4aed-8035-6117f7201b44 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b45f4928-00a7-4aed-8035-6117f7201b44');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0ec99b6a-d7e5-4de8-9efe-912aad7e03de\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ec99b6a-d7e5-4de8-9efe-912aad7e03de')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0ec99b6a-d7e5-4de8-9efe-912aad7e03de button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["   Unnamed: 0  SUBJECT_ID  ICD9_CODE  \\\n","0         958         112      41401   \n","1         959         112      41401   \n","2         965         114      41401   \n","3         980         115       4280   \n","4         982         115       4019   \n","\n","                                                TEXT GENDER  clean_len  \n","0  Admission Date: [**2194-6-13**]        Dischar...      M        948  \n","1  Admission Date:  [**2196-9-27**]              ...      M        881  \n","2  Admission Date:  [**2146-8-29**]       Dischar...      M        558  \n","3  Admission Date:  [**2194-10-16**]             ...      F       1552  \n","4  Admission Date:  [**2194-10-16**]             ...      F       1552  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKl7hK30zNi-"},"outputs":[],"source":["df.dropna(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eB-soY91zdcq"},"outputs":[],"source":["df.drop(['Unnamed: 0','GENDER', 'SUBJECT_ID','clean_len'],axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTuE56kvzgGj"},"outputs":[],"source":["df.rename(columns={'ICD9_CODE': 'label', 'TEXT': 'text'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCLlsmMQzi96"},"outputs":[],"source":["#Splitting\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n","\n","\n","#encoding label\n","encoder = LabelEncoder()\n","#train_set\n","train_df['label'] = encoder.fit_transform(train_df['label'])\n","#test_set\n","test_df['label'] = encoder.transform(test_df['label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu2XmX8MzmCL"},"outputs":[],"source":["train_df = train_df[['text', 'label']]\n","test_df = test_df[['text', 'label']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1704227207093,"user":{"displayName":"project","userId":"11180814093033221546"},"user_tz":-120},"id":"TtfX5RgG0Aqq","outputId":"f67cd4ff-cac3-4799-977d-04f7d9d0cd98"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 75102 entries, 71361 to 19666\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    75102 non-null  object\n"," 1   label   75102 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 1.7+ MB\n"]}],"source":["train_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1704227207093,"user":{"displayName":"project","userId":"11180814093033221546"},"user_tz":-120},"id":"X-xU2Ct10EFS","outputId":"9fb698f2-2d89-4f63-9481-677a3f7f38a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 18776 entries, 69532 to 70322\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    18776 non-null  object\n"," 1   label   18776 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 440.1+ KB\n"]}],"source":["test_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLd91U-Y0vLA"},"outputs":[],"source":["# train_df.to_csv('/content/drive/MyDrive/whole_dataset/5_train_whole_dataset_longformer.csv')\n","# test_df.to_csv('/content/drive/MyDrive/whole_dataset/5_test_whole_dataset_longformer.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D3SgKHDxOO2"},"outputs":[],"source":["# #using happytransformer\n","# #using RoBERTa(longformer)\n","# from happytransformer import HappyTextClassification, TCTrainArgs\n","# # --------------------------------------#\n","\n","# happy_tc = HappyTextClassification(model_type=\"LONGFORMER\", model_name=\"yikuan8/Clinical-Longformer\",\n","#                                  num_labels=5)\n","\n","# # Train the model\n","# args = TCTrainArgs(learning_rate = 1e-6, batch_size = 4, num_train_epochs = 20, save_steps= 0.1, output_dir = \"/content/drive/MyDrive/models_longformer\")\n","# happy_tc.train(\"/content/drive/MyDrive/whole_dataset/5_train_whole_dataset_longformer.csv\", args=args)\n","# happy_tc.save(\"finetuned_longformer_model/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FE93RiprxJ9v"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSkiS9VKlk9V"},"outputs":[],"source":["#run this code\n","#load model\n","from happytransformer import HappyTextClassification\n","\n","model = HappyTextClassification(model_type=\"BioBERT\", model_name=\"/content/drive/MyDrive/finetuned_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18024,"status":"ok","timestamp":1704824195526,"user":{"displayName":"project","userId":"11180814093033221546"},"user_tz":-120},"id":"8TRQj_jqJv5e","outputId":"d5681744-5ed0-4b5d-b60b-e9bfbb6b90b6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["ICD_CODE = 4280\n","accuracy: 0.58\n"]}],"source":["# # clipping approach\n","# from happytransformer import HappyTextClassification\n","# import re\n","# from collections import Counter, namedtuple\n","\n","# # Define a simple namedtuple to represent the classification result\n","# TextClassificationResult = namedtuple(\"TextClassificationResult\", [\"label\", \"score\"])\n","\n","# # Load your pre-trained model and tokenizer\n","# model = HappyTextClassification(model_type=\"BioBERT\", model_name=\"/content/drive/MyDrive/finetuned_model\")\n","# tokenizer = model.tokenizer\n","\n","# def prep_simple(pdf_read):\n","#     # Your existing preprocessing code\n","#     pdf_modified = re.sub(' +', ' ', pdf_read)\n","#     pdf_modified = pdf_modified.lower()\n","#     pdf_modified = pdf_modified.replace(\"\\n\", \" \")\n","#     pdf_modified = re.sub(r'\\[.*?\\]', '', pdf_modified)\n","#     pdf_modified = re.sub(' +', ' ', pdf_modified)\n","#     return pdf_modified.strip()\n","\n","# def tokenize_and_predict(text):\n","#     # Apply preprocessing\n","#     preprocessed_text = prep_simple(text)\n","\n","#     # Define the maximum token limit\n","#     max_token_limit = 512\n","\n","#     # Preprocess the text by splitting it into chunks\n","#     chunks = re.findall(r\"\\S+\\s*\", preprocessed_text)\n","#     chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n","\n","#     # Initialize predictions\n","#     predictions = []\n","\n","#     # Process each chunk\n","#     for chunk in chunks:\n","#         # Tokenize the chunk\n","#         # returns a list with every word or punctuation in a single string\n","#         tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(chunk)))\n","\n","#         # Ensure the chunk does not exceed the token limit\n","#         if len(tokens) <= max_token_limit:\n","#             # Make prediction for the chunk\n","#             prediction = model.classify_text(chunk)\n","#             predictions.append(prediction)\n","#         else:\n","#             print(f\"Warning: Chunk exceeds token limit and will be skipped:\\n{chunk}\")\n","\n","#     # Aggregate predictions (simple majority vote based on label and score)\n","#     aggregated_prediction = aggregate_predictions(predictions)\n","\n","#     return aggregated_prediction\n","\n","# def aggregate_predictions(predictions):\n","#     # Count occurrences of each label in the predictions\n","#     label_counts = Counter([result.label for result in predictions])\n","\n","#     # Find the label with the maximum count\n","#     majority_label = max(label_counts, key=label_counts.get)\n","\n","#     # Find the average score for the majority label\n","#     majority_score = sum([result.score for result in predictions if result.label == majority_label]) / label_counts[majority_label]\n","\n","#     # Create a TextClassificationResult object for the aggregated prediction\n","#     aggregated_prediction = TextClassificationResult(label=majority_label, score=majority_score)\n","\n","#     return aggregated_prediction\n","\n","# #predict text\n","# #label_0 = ICD_CODE(4280)\n","# #label_1 = ICD_CODE(41401)\n","# text_to_predict = text\n","# final_prediction = tokenize_and_predict(text_to_predict)\n","# #print label\n","# if final_prediction.label == 'LABEL_0':\n","#   print('ICD_CODE = 4280')\n","# elif final_prediction.label == 'LABEL_1':\n","#   print('ICD_CODE = 41401')\n","# #print score\n","# print(f\"accuracy: {final_prediction.score:.2f}\")\n","# #print(final_prediction.label)  # Access the label of the aggregated prediction\n","# #print(final_prediction.score)  # Access the score of the aggregated prediction"]},{"cell_type":"code","source":["#explainable AI\n","explainer = shap.TreeExplainer(your_trained_model)\n","shap_values = explainer.shap_values(your_data)\n","shap.summary_plot(shap_values, your_data)\n","shap.force_plot(explainer.expected_value, shap_values[0], your_data.iloc[0, :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xwki8JwPhG7h","executionInfo":{"status":"ok","timestamp":1704825584828,"user_tz":-120,"elapsed":2,"user":{"displayName":"project","userId":"11180814093033221546"}},"outputId":"3722b2a2-36f4-4f0d-f259-c9229e0769b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Logger happytransformer.happy_transformer (WARNING)>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["## run this code\n","from happytransformer import HappyTextClassification\n","from lime.lime_text import LimeTextExplainer\n","import re\n","from collections import Counter, namedtuple\n","\n","# Define a simple namedtuple to represent the classification result\n","TextClassificationResult = namedtuple(\"TextClassificationResult\", [\"label\", \"score\"])\n","\n","# Load your pre-trained model and tokenizer\n","model = HappyTextClassification(model_type=\"BioBERT\", model_name=\"/content/drive/MyDrive/finetuned_model\")\n","tokenizer = model.tokenizer\n","\n","def prep_simple(pdf_read):\n","    # Your existing preprocessing code\n","    pdf_modified = re.sub(' +', ' ', str(pdf_read))\n","    pdf_modified = pdf_modified.lower()\n","    pdf_modified = pdf_modified.replace(\"\\n\", \" \")\n","    pdf_modified = re.sub(r'\\[.*?\\]', '', pdf_modified)\n","    pdf_modified = re.sub(' +', ' ', pdf_modified)\n","    return pdf_modified.strip()\n","\n","def tokenize_and_predict(text):\n","    # Apply preprocessing\n","    preprocessed_text = prep_simple(text)\n","\n","    # Define the maximum token limit\n","    max_token_limit = 512\n","\n","    # Preprocess the text by splitting it into chunks\n","    chunks = re.findall(r\"\\S+\\s*\", preprocessed_text)\n","    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n","\n","    # Initialize predictions\n","    predictions = []\n","\n","    # Process each chunk\n","    for chunk in chunks:\n","        # Tokenize the chunk\n","        # returns a list with every word or punctuation in a single string\n","        tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(chunk)))\n","\n","        # Ensure the chunk does not exceed the token limit\n","        if len(tokens) <= max_token_limit:\n","            # Make prediction for the chunk\n","            prediction = model.classify_text(chunk)\n","            predictions.append(prediction)\n","        else:\n","            print(f\"Warning: Chunk exceeds token limit and will be skipped:\\n{chunk}\")\n","\n","    # Aggregate predictions (simple majority vote based on label and score)\n","    aggregated_prediction = aggregate_predictions(predictions)\n","\n","    return aggregated_prediction\n","\n","def aggregate_predictions(predictions):\n","    # Count occurrences of each label in the predictions\n","    label_counts = Counter([result.label for result in predictions])\n","\n","    # Find the label with the maximum count\n","    majority_label = max(label_counts, key=label_counts.get)\n","\n","    # Find the average score for the majority label\n","    majority_score = sum([result.score for result in predictions if result.label == majority_label]) / label_counts[majority_label]\n","\n","    # Create a TextClassificationResult object for the aggregated prediction\n","    aggregated_prediction = TextClassificationResult(label=majority_label, score=majority_score)\n","\n","    return aggregated_prediction\n","\n","\n","############################### explain text function\n","##### tokenize and predict--> (classifier function(model used for classifing text if it is ICD code 4280 or 41401))\n","def explain_text(text):\n","    # Create LIME explainer\n","    explainer = LimeTextExplainer(class_names=[0, 1])\n","\n","    # Explain the model's prediction for the given text\n","    explanation = explainer.explain_instance(text, tokenize_and_predict, num_features=30)\n","\n","    # Print the explanation\n","    print(explanation.as_list())\n","\n","# Example usage\n","text = \"\"\"Unit No:  [**Numeric Identifier 56787**]\\nAdmission Date: [**2119-11-14**]\\nDischarge Date: [**2119-12-1**]\\nDate of Birth:  [**2058-6-3**]\\nSex:  M\\nService:  CSU\\n\\n\\nHISTORY OF PRESENT ILLNESS:  Father [**Name (NI) **] is a 61-year-old\\nman, with known CAD, status post coronary artery bypass graft\\non [**2119-10-31**] with a LIMA to the LAD, saphenous vein graft to\\nOM1, saphenous vein graft to D1, and saphenous vein graft to\\nPDA.  The patient was discharged home on [**11-5**], and returns\\non the day of admission complaining of sternal drainage x\\nseveral days with increasing amounts on the day of admission.\\nThe patient denies fever, chills, nausea, vomiting, or\\nmalaise.\\n\\nPAST MEDICAL HISTORY:  CAD, status post CABG with an EF of 20\\npercent.\\n\\nDiabetes mellitus, currently insulin dependent.\\n\\nHypercholesterolemia.\\n\\nGERD.\\n\\nALLERGIES:  None.\\n\\nMEDS ON ADMISSION:\\n1. Colace 100 mg [**Hospital1 **].\\n2. Aspirin 81 mg once daily.\\n3. Plavix 75 mg once daily.\\n4. Carvedilol 6.25 mg [**Hospital1 **].\\n5. Simvastatin 40 mg once daily.\\n6. Lasix 40 mg [**Hospital1 **].\\n7. Lantus insulin 45 units q pm.\\n8. Percocet 5/325, 1-2 tabs q 4 h prn.\\n\\n\\nLABS ON ADMISSION:  White count 18.6, hematocrit 33.9,\\nplatelets 893, PT 17.5, PTT 24, INR 1.1, sodium 139,\\npotassium 4.2, chloride 101, CO2 25, BUN 14, creatinine 0.9,\\nglucose 246.  Chest x-ray shows cardiomegaly with left-sided\\neffusion with atelectasis, multiple displaced wires.  EKG:\\nSinus rhythm with a rate of 100, Q's in III and AVF,\\nnonspecific ST changes with poor R wave progression.\\n\\nPHYSICAL EXAM:  Temperature 103, heart rate 116--sinus\\ntachycardia, blood pressure 100/47, respiratory rate 30, O2\\nsat 97 percent on 2 liters nasal prongs.  Neuro:  Alert and\\noriented x 3, moves all extremities, follows commands,\\nnonfocal exam.  Respiratory:  Clear to auscultation with a\\nsucking chest wound.  Cardiovascular:  Regular rate and\\nrhythm.  Sternum with surrounding erythema of about 10 cm,\\nwith a positive click.  Small draining hole in midincision\\nwith milky serous drainage.  Staples remain in place.\\nAbdomen is soft, nontender, nondistended with normoactive\\nbowel sounds.  Extremities are warm and well-perfused with no\\nedema.  Right calf with a healing wound and minimal erythema.\\nLeft knee with an endoscopic site that is healing, open to\\nair, clean and dry.\\n\\nHOSPITAL COURSE:  The patient was admitted to the\\nCardiothoracic Intensive Care Unit.  He was begun on\\nvancomycin 1 gm q 12 h, as well as levofloxacin 500 mg once\\ndaily.  He was typed and screened and kept NPO for\\nmediastinal exploration plus/minus a flap closure.\\n\\nOn hospital day 2, the patient was brought to the operating\\nroom.  Please see the OR report for full details.  In\\nsummary, the patient had a sternal exploration and\\ndebridement.  He tolerated the operation well and was\\nreturned to the Cardiothoracic Intensive Care Unit intubated\\nand sedated with an open chest wound.  Plastic surgery was\\nalso following the patient.\\n\\nThe patient did well in the immediate postoperative period.\\nHis anesthesia was reversed.  He was weaned from the\\nventilator and successfully extubated.  Several hours\\nfollowing extubation, the patient was found to be in acute\\nrespiratory distress and was emergently reintubated.  From\\nthat point forward, he was kept sedated and ventilated\\nawaiting plastics follow-up for flap closure.\\n\\nOn the [**11-19**], the patient returned to the operating\\nroom.  Please see the OR report for full details.  In\\nsummary, the patient was brought to the operating room by the\\nplastic surgery service for pectoral advancement with an\\nomentum flap.  He tolerated the operation well and was\\nreturned to the Cardiothoracic Intensive Care Unit.  The\\npatient remained intubated following his surgery.  However,\\nhis sedation was minimized to allow the patient to\\noverbreathe the ventilator.  During that period, the patient\\nhad several episodes of coughing which led to a dehiscence of\\nhis abdominal incision, and on the [**11-20**] the patient\\nagain returned to the operating room for re-exploration and\\nclosure of the fascia of his abdominal wound.  He tolerated\\nthis surgery well also and following that returned to the\\nCardiothoracic Intensive Care Unit, again ventilated and\\nsedated.  The patient remained ventilated and sedated for the\\nnext several days in an attempt to give the wound a chance to\\nheal.\\n\\nUltimately, the patient was successfully extubated on the [**11-24**].  However, he stayed in the Cardiothoracic\\nIntensive Care Unit following extubation for close pulmonary\\nmonitoring.  It should be noted that during the patient's ICU\\ncourse, he had several intermittent episodes of atrial\\nfibrillation for which he was begun on amiodarone, as well as\\nheparin and ultimately Coumadin for anticoagulation.  The\\npatient did well over the next several days, and ultimately\\nwas transferred to the floor on [**11-28**], hospital day 15,\\npostoperative day 13.  At that point, a PICC line was placed\\nfor long-term antibiotic coverage.\\n\\nOver the next several days, the patient's activity level was\\nincreased with the assistance of the nursing and the physical\\ntherapy staff.  His antibiotic coverage was continued.  His\\nanticoagulation was transitioned from intravenous to oral.\\nFinally, on the [**12-1**], the patient's final [**Location (un) 1661**]-\\n[**Location (un) 1662**] drain was removed from his chest, and it was decided\\nthat he was stable and ready to be transferred to\\nrehabilitation for long-term antibiotic coverage, as well as\\nglucose control.\\n\\nAt that time, the patient's physical exam was as follows:\\nVital signs:  Temperature 98.4, heart rate 82--sinus rhythm,\\nblood pressure 113/66, respiratory rate 18, O2 sat 95 percent\\non room air, weight day of dictation 106.6 kg, preoperatively\\n100 kg.  Lab data:  PT 17.1, INR 1.9, sodium 139, potassium\\n3.7, chloride 100, bicarb 27, BUN 11, creatinine 0.9, glucose\\n149, white count 9.1, hematocrit 28.4, platelets 830.\\nPhysical exam - Neurologically:  Alert and oriented x 3,\\nnonfocal exam.  Pulmonary:  Clear to auscultation\\nbilaterally.  Cardiac:  Regular rate and rhythm, S1, S2.\\nSternum:  Incision with staples, clean and dry.  No erythema\\nor drainage.  Abdomen was soft, nontender, nondistended with\\nnormoactive bowel sounds.  Abdominal incision with staples,\\nalso clean and dry.  Extremities were warm with no edema.\\nRight saphenous vein graft harvest site was healing well,\\nopen to air, clean and dry.\\n\\nCONDITION ON DISCHARGE:  Good.\\n\\nDISCHARGE DIAGNOSES:  Coronary artery disease, status post\\ncoronary artery bypass grafting complicated by sternal\\ninfection requiring sternal debridement and flap closure.\\n\\nDiabetes mellitus.\\n\\nHypercholesterolemia.\\n\\nGastroesophageal reflux disease.\\n\\nFOLLOW UP:  Follow-up with Dr. [**Last Name (STitle) 13797**] with plastic surgery\\nservice in 1 week.  He is to call for an appointment at [**Telephone/Fax (1) 56789**].  He is also to have follow-up with Dr. [**Last Name (STitle) **] in 4\\nweeks.  The patient is also to call for that appointment; the\\nnumber is [**Telephone/Fax (1) 170**].\\n\\nDISCHARGE MEDICATIONS:\\n1. Ranitidine 150 mg [**Hospital1 **].\\n2. Simvastatin 40 mg once daily.\\n3. Ferrous sulfate 325 mg once daily.\\n4. Ascorbic acid 500 mg [**Hospital1 **].\\n5. Zinc sulfate 220 mg once daily.\\n6. Aspirin 81 mg once daily.\\n7. Erythromycin ophthalmic ointment [**Hospital1 **].\\n8. Colace 100 mg [**Hospital1 **].\\n9. Metoprolol XL 100 mg once daily.\\n10.Glargine 24 units q at bedtime.\\n11.Humalog insulin sliding scale q ac and at bedtime.\\n12.Lasix 20 mg once daily.\\n13.Potassium chloride 20 mEq once daily.\\n14.Amiodarone 400 mg [**Hospital1 **] x 1 week, then 400 mg once daily x 1\\nweek, then 200 mg once daily.\\n15.Oxacillin 2 grams q 4 h through [**12-28**].\\n16.Warfarin as directed to maintain a target INR of 2 to 2.5.\\nThe patient's warfarin doses starting with 4 days ago - 3 mg,\\n5 mg, 5 mg, 5 mg.  The patient is to receive 4 mg on the [**2032-11-29**].Albuterol 2 puffs qid prn.\\n\\nDISPOSITION:  The patient is to be discharged to\\nrehabilitation.\\n\\n\\n\\n                        [**First Name11 (Name Pattern1) **] [**Last Name (NamePattern1) 1714**], [**MD Number(1) 1715**]\\n\\nDictated By:[**Last Name (NamePattern4) 1718**]\\nMEDQUIST36\\nD:  [**2119-12-1**] 13:31:57\\nT:  [**2119-12-1**] 14:15:12\\nJob#:  [**Job Number 56790**]\\n\\n\\n\\n\"\"\"\n","final_prediction = tokenize_and_predict(text)\n","\n","# Print label and score\n","if final_prediction.label == 'LABEL_0':\n","    prediction_label = 0\n","    print('ICD_CODE = 4280')\n","elif final_prediction.label == 'LABEL_1':\n","    prediction_label = 1\n","    print('ICD_CODE = 41401')\n","\n","print(f\"Score: {final_prediction.score:.2f}\")\n","\n","# Explain the model's prediction\n","explain_text(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"iXinKYIzf3Nh","executionInfo":{"status":"error","timestamp":1704828712503,"user_tz":-120,"elapsed":1632603,"user":{"displayName":"project","userId":"11180814093033221546"}},"outputId":"a81f9ea3-fff5-4ac4-8ba9-5139efee0521"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["ICD_CODE = 4280\n","Accuracy: 0.58\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|‚ñè         | 3784/211857 [59:49<54:49:49,  1.05it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-43392849a625>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Explain the model's prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mexplain_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-43392849a625>\u001b[0m in \u001b[0;36mexplain_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Explain the model's prediction for the given text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_and_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Print the explanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lime/lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    411\u001b[0m                                         mask_string=self.mask_string))\n\u001b[1;32m    412\u001b[0m         \u001b[0mdomain_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDomainMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         data, yss, distances = self.__data_labels_distances(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             distance_metric=distance_metric)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lime/lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[0;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-43392849a625>\u001b[0m in \u001b[0;36mtokenize_and_predict\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Make prediction for the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/happytransformer/happy_text_classification.py\u001b[0m in \u001b[0;36mclassify_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the \\\"text\\\" argument must be a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# we do not support predicting a list of  texts, so only first prediction is relevant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mfirst_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mper\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0m_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             )\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1565\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         )\n\u001b[0;32m-> 1013\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 )\n\u001b[1;32m    606\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    608\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 427\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kVlkfo976s4"},"outputs":[],"source":["text = \"\"\"Unit No:  [**Numeric Identifier 56787**]\\nAdmission Date: [**2119-11-14**]\\nDischarge Date: [**2119-12-1**]\\nDate of Birth:  [**2058-6-3**]\\nSex:  M\\nService:  CSU\\n\\n\\nHISTORY OF PRESENT ILLNESS:  Father [**Name (NI) **] is a 61-year-old\\nman, with known CAD, status post coronary artery bypass graft\\non [**2119-10-31**] with a LIMA to the LAD, saphenous vein graft to\\nOM1, saphenous vein graft to D1, and saphenous vein graft to\\nPDA.  The patient was discharged home on [**11-5**], and returns\\non the day of admission complaining of sternal drainage x\\nseveral days with increasing amounts on the day of admission.\\nThe patient denies fever, chills, nausea, vomiting, or\\nmalaise.\\n\\nPAST MEDICAL HISTORY:  CAD, status post CABG with an EF of 20\\npercent.\\n\\nDiabetes mellitus, currently insulin dependent.\\n\\nHypercholesterolemia.\\n\\nGERD.\\n\\nALLERGIES:  None.\\n\\nMEDS ON ADMISSION:\\n1. Colace 100 mg [**Hospital1 **].\\n2. Aspirin 81 mg once daily.\\n3. Plavix 75 mg once daily.\\n4. Carvedilol 6.25 mg [**Hospital1 **].\\n5. Simvastatin 40 mg once daily.\\n6. Lasix 40 mg [**Hospital1 **].\\n7. Lantus insulin 45 units q pm.\\n8. Percocet 5/325, 1-2 tabs q 4 h prn.\\n\\n\\nLABS ON ADMISSION:  White count 18.6, hematocrit 33.9,\\nplatelets 893, PT 17.5, PTT 24, INR 1.1, sodium 139,\\npotassium 4.2, chloride 101, CO2 25, BUN 14, creatinine 0.9,\\nglucose 246.  Chest x-ray shows cardiomegaly with left-sided\\neffusion with atelectasis, multiple displaced wires.  EKG:\\nSinus rhythm with a rate of 100, Q's in III and AVF,\\nnonspecific ST changes with poor R wave progression.\\n\\nPHYSICAL EXAM:  Temperature 103, heart rate 116--sinus\\ntachycardia, blood pressure 100/47, respiratory rate 30, O2\\nsat 97 percent on 2 liters nasal prongs.  Neuro:  Alert and\\noriented x 3, moves all extremities, follows commands,\\nnonfocal exam.  Respiratory:  Clear to auscultation with a\\nsucking chest wound.  Cardiovascular:  Regular rate and\\nrhythm.  Sternum with surrounding erythema of about 10 cm,\\nwith a positive click.  Small draining hole in midincision\\nwith milky serous drainage.  Staples remain in place.\\nAbdomen is soft, nontender, nondistended with normoactive\\nbowel sounds.  Extremities are warm and well-perfused with no\\nedema.  Right calf with a healing wound and minimal erythema.\\nLeft knee with an endoscopic site that is healing, open to\\nair, clean and dry.\\n\\nHOSPITAL COURSE:  The patient was admitted to the\\nCardiothoracic Intensive Care Unit.  He was begun on\\nvancomycin 1 gm q 12 h, as well as levofloxacin 500 mg once\\ndaily.  He was typed and screened and kept NPO for\\nmediastinal exploration plus/minus a flap closure.\\n\\nOn hospital day 2, the patient was brought to the operating\\nroom.  Please see the OR report for full details.  In\\nsummary, the patient had a sternal exploration and\\ndebridement.  He tolerated the operation well and was\\nreturned to the Cardiothoracic Intensive Care Unit intubated\\nand sedated with an open chest wound.  Plastic surgery was\\nalso following the patient.\\n\\nThe patient did well in the immediate postoperative period.\\nHis anesthesia was reversed.  He was weaned from the\\nventilator and successfully extubated.  Several hours\\nfollowing extubation, the patient was found to be in acute\\nrespiratory distress and was emergently reintubated.  From\\nthat point forward, he was kept sedated and ventilated\\nawaiting plastics follow-up for flap closure.\\n\\nOn the [**11-19**], the patient returned to the operating\\nroom.  Please see the OR report for full details.  In\\nsummary, the patient was brought to the operating room by the\\nplastic surgery service for pectoral advancement with an\\nomentum flap.  He tolerated the operation well and was\\nreturned to the Cardiothoracic Intensive Care Unit.  The\\npatient remained intubated following his surgery.  However,\\nhis sedation was minimized to allow the patient to\\noverbreathe the ventilator.  During that period, the patient\\nhad several episodes of coughing which led to a dehiscence of\\nhis abdominal incision, and on the [**11-20**] the patient\\nagain returned to the operating room for re-exploration and\\nclosure of the fascia of his abdominal wound.  He tolerated\\nthis surgery well also and following that returned to the\\nCardiothoracic Intensive Care Unit, again ventilated and\\nsedated.  The patient remained ventilated and sedated for the\\nnext several days in an attempt to give the wound a chance to\\nheal.\\n\\nUltimately, the patient was successfully extubated on the [**11-24**].  However, he stayed in the Cardiothoracic\\nIntensive Care Unit following extubation for close pulmonary\\nmonitoring.  It should be noted that during the patient's ICU\\ncourse, he had several intermittent episodes of atrial\\nfibrillation for which he was begun on amiodarone, as well as\\nheparin and ultimately Coumadin for anticoagulation.  The\\npatient did well over the next several days, and ultimately\\nwas transferred to the floor on [**11-28**], hospital day 15,\\npostoperative day 13.  At that point, a PICC line was placed\\nfor long-term antibiotic coverage.\\n\\nOver the next several days, the patient's activity level was\\nincreased with the assistance of the nursing and the physical\\ntherapy staff.  His antibiotic coverage was continued.  His\\nanticoagulation was transitioned from intravenous to oral.\\nFinally, on the [**12-1**], the patient's final [**Location (un) 1661**]-\\n[**Location (un) 1662**] drain was removed from his chest, and it was decided\\nthat he was stable and ready to be transferred to\\nrehabilitation for long-term antibiotic coverage, as well as\\nglucose control.\\n\\nAt that time, the patient's physical exam was as follows:\\nVital signs:  Temperature 98.4, heart rate 82--sinus rhythm,\\nblood pressure 113/66, respiratory rate 18, O2 sat 95 percent\\non room air, weight day of dictation 106.6 kg, preoperatively\\n100 kg.  Lab data:  PT 17.1, INR 1.9, sodium 139, potassium\\n3.7, chloride 100, bicarb 27, BUN 11, creatinine 0.9, glucose\\n149, white count 9.1, hematocrit 28.4, platelets 830.\\nPhysical exam - Neurologically:  Alert and oriented x 3,\\nnonfocal exam.  Pulmonary:  Clear to auscultation\\nbilaterally.  Cardiac:  Regular rate and rhythm, S1, S2.\\nSternum:  Incision with staples, clean and dry.  No erythema\\nor drainage.  Abdomen was soft, nontender, nondistended with\\nnormoactive bowel sounds.  Abdominal incision with staples,\\nalso clean and dry.  Extremities were warm with no edema.\\nRight saphenous vein graft harvest site was healing well,\\nopen to air, clean and dry.\\n\\nCONDITION ON DISCHARGE:  Good.\\n\\nDISCHARGE DIAGNOSES:  Coronary artery disease, status post\\ncoronary artery bypass grafting complicated by sternal\\ninfection requiring sternal debridement and flap closure.\\n\\nDiabetes mellitus.\\n\\nHypercholesterolemia.\\n\\nGastroesophageal reflux disease.\\n\\nFOLLOW UP:  Follow-up with Dr. [**Last Name (STitle) 13797**] with plastic surgery\\nservice in 1 week.  He is to call for an appointment at [**Telephone/Fax (1) 56789**].  He is also to have follow-up with Dr. [**Last Name (STitle) **] in 4\\nweeks.  The patient is also to call for that appointment; the\\nnumber is [**Telephone/Fax (1) 170**].\\n\\nDISCHARGE MEDICATIONS:\\n1. Ranitidine 150 mg [**Hospital1 **].\\n2. Simvastatin 40 mg once daily.\\n3. Ferrous sulfate 325 mg once daily.\\n4. Ascorbic acid 500 mg [**Hospital1 **].\\n5. Zinc sulfate 220 mg once daily.\\n6. Aspirin 81 mg once daily.\\n7. Erythromycin ophthalmic ointment [**Hospital1 **].\\n8. Colace 100 mg [**Hospital1 **].\\n9. Metoprolol XL 100 mg once daily.\\n10.Glargine 24 units q at bedtime.\\n11.Humalog insulin sliding scale q ac and at bedtime.\\n12.Lasix 20 mg once daily.\\n13.Potassium chloride 20 mEq once daily.\\n14.Amiodarone 400 mg [**Hospital1 **] x 1 week, then 400 mg once daily x 1\\nweek, then 200 mg once daily.\\n15.Oxacillin 2 grams q 4 h through [**12-28**].\\n16.Warfarin as directed to maintain a target INR of 2 to 2.5.\\nThe patient's warfarin doses starting with 4 days ago - 3 mg,\\n5 mg, 5 mg, 5 mg.  The patient is to receive 4 mg on the [**2032-11-29**].Albuterol 2 puffs qid prn.\\n\\nDISPOSITION:  The patient is to be discharged to\\nrehabilitation.\\n\\n\\n\\n                        [**First Name11 (Name Pattern1) **] [**Last Name (NamePattern1) 1714**], [**MD Number(1) 1715**]\\n\\nDictated By:[**Last Name (NamePattern4) 1718**]\\nMEDQUIST36\\nD:  [**2119-12-1**] 13:31:57\\nT:  [**2119-12-1**] 14:15:12\\nJob#:  [**Job Number 56790**]\\n\\n\\n\\n\"\"\""]},{"cell_type":"code","source":["# from happytransformer import HappyTextClassification\n","# import re\n","# from collections import Counter, namedtuple\n","\n","# # Define a simple namedtuple to represent the classification result\n","# TextClassificationResult = namedtuple(\"TextClassificationResult\", [\"label\", \"score\"])\n","\n","# # Load your pre-trained model and tokenizer\n","# model = HappyTextClassification(model_type=\"BioBERT\", model_name=\"/content/drive/MyDrive/finetuned_model\")\n","# tokenizer = model.tokenizer\n","\n","# def prep_simple(pdf_read):\n","#     # Your existing preprocessing code\n","#     pdf_modified = re.sub(' +', ' ', pdf_read)\n","#     pdf_modified = pdf_modified.lower()\n","#     pdf_modified = pdf_modified.replace(\"\\n\", \" \")\n","#     pdf_modified = re.sub(r'\\[.*?\\]', '', pdf_modified)\n","#     pdf_modified = re.sub(' +', ' ', pdf_modified)\n","#     return pdf_modified.strip()\n","\n","# def tokenize_and_predict(text, max_words_per_chunk=55):\n","#     # Apply preprocessing\n","#     preprocessed_text = prep_simple(text)\n","\n","#     # Tokenize the text into words\n","#     tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(preprocessed_text)))\n","\n","#     # Calculate the total number of words in the text\n","#     total_words = len(tokens)\n","\n","#     # Determine the number of full chunks and the remaining words\n","#     num_full_chunks = total_words // max_words_per_chunk\n","#     remaining_words = total_words % max_words_per_chunk\n","\n","#     # Initialize predictions\n","#     predictions = []\n","\n","#     # Process each full chunk\n","#     for i in range(num_full_chunks):\n","#         start_idx = i * max_words_per_chunk\n","#         end_idx = (i + 1) * max_words_per_chunk\n","#         chunk = \" \".join(tokens[start_idx:end_idx])\n","\n","#         # Make prediction for the chunk\n","#         prediction = model.classify_text(chunk)\n","#         predictions.append(prediction)\n","\n","#     # Process the remaining chunk\n","#     if remaining_words > 0:\n","#         start_idx = num_full_chunks * max_words_per_chunk\n","#         end_idx = total_words\n","#         remaining_chunk = \" \".join(tokens[start_idx:end_idx])\n","\n","#         # Make prediction for the remaining chunk\n","#         prediction = model.classify_text(remaining_chunk)\n","#         predictions.append(prediction)\n","\n","#     # Aggregate predictions (simple majority vote based on label and score)\n","#     aggregated_prediction = aggregate_predictions(predictions)\n","\n","#     return aggregated_prediction\n","\n","# def aggregate_predictions(predictions):\n","#     # Count occurrences of each label in the predictions\n","#     label_counts = Counter([result.label for result in predictions])\n","\n","#     # Find the label with the maximum count\n","#     majority_label = max(label_counts, key=label_counts.get)\n","\n","#     # Find the average score for the majority label\n","#     majority_score = sum([result.score for result in predictions if result.label == majority_label]) / label_counts[majority_label]\n","\n","#     # Create a TextClassificationResult object for the aggregated prediction\n","#     aggregated_prediction = TextClassificationResult(label=majority_label, score=majority_score)\n","\n","#     return aggregated_prediction\n","\n","# # Example usage\n","# text_to_predict = \"Your long text here...\"\n","# final_prediction = tokenize_and_predict(text_to_predict)\n","# print(final_prediction.label)  # Access the label of the aggregated prediction\n","# print(final_prediction.score)  # Access the score of the aggregated prediction\n"],"metadata":{"id":"Z1A8nByInE4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-xbkbgiWEfz"},"outputs":[],"source":["#Test\n","# test_result = model.test(\"/content/drive/MyDrive/processed_datasets/2_whole_Dataset_test_df_60(50)_unlabelled.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvHm-1Qfb52C"},"outputs":[],"source":["# print(type(test_result))\n","# print(test_result)\n","# print(type(test_result[0]))\n","# print(test_result[0])\n","# print(test_result[0].label)"]},{"cell_type":"code","source":["#classification report representing model performance\n","# from sklearn.metrics import classification_report\n","\n","# true_labels = [example[\"label\"] for example in test_data]\n","# predicted_labels = predictions.labels\n","\n","# report = classification_report(true_labels, predicted_labels)"],"metadata":{"id":"XY681t6qm1N-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhGwiVPJUf8c"},"outputs":[],"source":["# Import necessary libraries\n","#deployment using streamlit\n","import streamlit as st\n","from happytransformer import HappyTextClassification\n","\n","# Load the pre-trained text classification model\n","model = HappyTextClassification.from_pretrained(\"/content/drive/MyDrive/finetuned_model\")\n","\n","# Streamlit app\n","st.title(\"Text Classification App\")\n","\n","# User input\n","user_input = st.text_area(\"Enter text for classification:\")\n","\n","# Make prediction when the user clicks the \"Classify\" button\n","if st.button(\"Classify\"):\n","    # Generate prediction\n","    prediction = model.classify_text(user_input)\n","\n","    # Display the prediction result\n","    st.write(\"Prediction:\")\n","    st.write(f\"Class: {prediction.label}\")\n","    st.write(f\"Confidence: {prediction.score:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0u1g64DNzxC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}